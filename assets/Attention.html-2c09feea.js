const t=JSON.parse(`{"key":"v-cd07ac56","path":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Attention.html","title":"Self_Attention","lang":"en-US","frontmatter":{"title":"Self_Attention","date":"2023-03-01T00:00:00.000Z","tag":["Deep Learning","Attention"],"category":["深度学习"],"description":"Attention机制 Info 个人理解注意力机制是一种特殊的全连接层，其中一共有若干个（多头）Wq，Wk，Wv三个矩阵作为注意力参数（每一个矩阵大小都是n特征维数的平方） 1 Self-attention 以a1得到b1为例讲解注意力机制：","head":[["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://Huahuatii.github.io/zh/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Attention.html"}],["meta",{"property":"og:url","content":"https://Huahuatii.github.io/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Attention.html"}],["meta",{"property":"og:site_name","content":"Huahuatii's Blog"}],["meta",{"property":"og:title","content":"Self_Attention"}],["meta",{"property":"og:description","content":"Attention机制 Info 个人理解注意力机制是一种特殊的全连接层，其中一共有若干个（多头）Wq，Wk，Wv三个矩阵作为注意力参数（每一个矩阵大小都是n特征维数的平方） 1 Self-attention 以a1得到b1为例讲解注意力机制："}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://Huahuatii.github.io/"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:alt","content":"Self_Attention"}],["meta",{"property":"article:tag","content":"Deep Learning"}],["meta",{"property":"article:tag","content":"Attention"}],["meta",{"property":"article:published_time","content":"2023-03-01T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Self_Attention\\",\\"image\\":[\\"https://Huahuatii.github.io/\\"],\\"datePublished\\":\\"2023-03-01T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[]}"]]},"headers":[{"level":2,"title":"1 Self-attention","slug":"_1-self-attention","link":"#_1-self-attention","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":2.37,"words":710},"filePathRelative":"posts/深度学习/Attention.md","localizedDate":"March 1, 2023","excerpt":"\\n<h1> Attention机制</h1>\\n<div class=\\"hint-container info\\">\\n<p class=\\"hint-container-title\\">Info</p>\\n<p>个人理解注意力机制是一种特殊的全连接层，其中一共有若干个（多头）W<sub>q</sub>，W<sub>k</sub>，W<sub>v</sub>三个矩阵作为注意力参数（每一个矩阵大小都是<sup>n</sup>特征维数的平方）</p>\\n</div>\\n<h2> 1 Self-attention</h2>\\n<p>以a<sup>1</sup>得到b<sup>1</sup>为例讲解注意力机制：</p>","autoDesc":true}`);export{t as data};
